{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cf2114",
   "metadata": {},
   "source": [
    "### **1 - Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b6305",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:06:26.790116Z",
     "iopub.status.busy": "2025-05-05T16:06:26.789199Z",
     "iopub.status.idle": "2025-05-05T16:06:59.798875Z",
     "shell.execute_reply": "2025-05-05T16:06:59.798077Z",
     "shell.execute_reply.started": "2025-05-05T16:06:26.790083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067cf2c",
   "metadata": {},
   "source": [
    "### **2 - Loading Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6fdf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:06:59.799862Z",
     "iopub.status.busy": "2025-05-05T16:06:59.799645Z",
     "iopub.status.idle": "2025-05-05T16:06:59.808436Z",
     "shell.execute_reply": "2025-05-05T16:06:59.807591Z",
     "shell.execute_reply.started": "2025-05-05T16:06:59.799845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# general \n",
    "HGF = config['general']['HGF']\n",
    "\n",
    "# output model\n",
    "output_model_online_Desc = config['outputs']['output_model_online_Desc']\n",
    "output_model_online_Rec = config['outputs']['output_model_online_Rec']\n",
    "\n",
    "# model\n",
    "max_seq_length = config['model']['max_seq_length']\n",
    "load_in_4bit = config['model']['load_in_4bit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbfb0a6",
   "metadata": {},
   "source": [
    "### **3- Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5e73c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:06:59.811049Z",
     "iopub.status.busy": "2025-05-05T16:06:59.810763Z",
     "iopub.status.idle": "2025-05-05T16:07:00.395103Z",
     "shell.execute_reply": "2025-05-05T16:07:00.394348Z",
     "shell.execute_reply.started": "2025-05-05T16:06:59.811025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MRR\n",
    "def mean_reciprocal_rank(predicted, real):\n",
    "    ranks = []\n",
    "    \n",
    "    for real_value, predicted_list in zip(real, predicted):\n",
    "        try:\n",
    "            rank = predicted_list.index(real_value) + 1  # Rank is 1-based\n",
    "            ranks.append(1 / rank)\n",
    "        except ValueError:\n",
    "            ranks.append(0)  # If real_value is not in predicted_list\n",
    "    \n",
    "    return (sum(ranks) / len(ranks)) * 100  # Convert to percentage\n",
    "\n",
    "# NDCG\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\" Compute DCG@k given a binary relevance list r (1 if relevant, 0 otherwise). \"\"\"\n",
    "    r = np.array(r[:k])  # Consider only top-k predictions\n",
    "    return np.sum(r / np.log2(np.arange(1, len(r) + 1) + 1))\n",
    "\n",
    "def ndcg_at_k(predicted, real, k=5):\n",
    "    \"\"\" Compute nDCG@k for a list of predicted rankings and real labels. \"\"\"\n",
    "    ndcgs = []\n",
    "\n",
    "    for real_value, predicted_list in zip(real, predicted):\n",
    "        # Relevance vector: 1 if correct, 0 otherwise\n",
    "        relevance = [1 if c == real_value else 0 for c in predicted_list[:k]]\n",
    "\n",
    "        # Compute DCG and IDCG\n",
    "        dcg = dcg_at_k(relevance, k)\n",
    "        idcg = dcg_at_k([1] * min(k, 1), k)  \n",
    "\n",
    "        # Compute nDCG\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        ndcgs.append(ndcg)\n",
    "\n",
    "    return np.mean(ndcgs) * 100  # Convert to percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2e7c2",
   "metadata": {},
   "source": [
    "### **4 - Loading Data and models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9728224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:07:00.396853Z",
     "iopub.status.busy": "2025-05-05T16:07:00.396080Z",
     "iopub.status.idle": "2025-05-05T16:07:52.226247Z",
     "shell.execute_reply": "2025-05-05T16:07:52.225201Z",
     "shell.execute_reply.started": "2025-05-05T16:07:00.396826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"Data/test.csv\", index_col=0)\n",
    "\n",
    "\n",
    "model_d, tokenizer_d = FastLanguageModel.from_pretrained(\n",
    "    model_name = output_model_online_Desc,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = HGF\n",
    ")\n",
    "\n",
    "\n",
    "model_r, tokenizer_r = FastLanguageModel.from_pretrained(\n",
    "    model_name = output_model_online_Rec,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = HGF\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2399f0",
   "metadata": {},
   "source": [
    "### **5 - Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30100d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptDesc = \"\"\" Below is an instruction that describes a task, paired with an input that provied further context.\n",
    "Write a response that appropiately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an interests analyzer. Based on the following user history, analyze their reading habits and generate a description of what kind of news articles they might be interested in reading next. \n",
    "\n",
    "### History:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "Description : \\n\n",
    "{}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f25e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:07:52.227861Z",
     "iopub.status.busy": "2025-05-05T16:07:52.227169Z",
     "iopub.status.idle": "2025-05-05T16:07:52.231744Z",
     "shell.execute_reply": "2025-05-05T16:07:52.230695Z",
     "shell.execute_reply.started": "2025-05-05T16:07:52.227841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "promptRec = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You serve as a personalized news article recommendation system. Based on the user's preference descriptions below and the candidate articles, rank the candidates using their labels.\n",
    "Output Format:\n",
    "Ranked News Articles: <START> C#, C#, ..., C# <END>\n",
    "\n",
    "### Preferences Description:\n",
    "{}\n",
    "\n",
    "### Candidates:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{} \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606514fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Descriptions(df,model,tokenizer):\n",
    "\n",
    "    history = []\n",
    "    desc = []\n",
    "    candidates = []\n",
    "    labels = []\n",
    "\n",
    "    for (i, row) in df.iterrows():\n",
    "           \n",
    "            prompt = row['history']\n",
    "            inputs = tokenizer([promptDesc.format(prompt, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens= 900\n",
    "            )\n",
    "            response = tokenizer.batch_decode(outputs)\n",
    "            result = response[0].split(\"### Response:\")[1].split(\"\\nDescription : \\n\\n\\n\\n\")[1].replace(\"<｜end▁of▁sentence｜>\",\"\")\n",
    "            \n",
    "            desc.append(result)\n",
    "            candidates.append(row['candidate'])\n",
    "            labels.append(row['label'])\n",
    "            history.append(prompt)\n",
    "            \n",
    "    return history, desc, candidates, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fd7f87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:07:52.232730Z",
     "iopub.status.busy": "2025-05-05T16:07:52.232486Z",
     "iopub.status.idle": "2025-05-05T16:07:55.163657Z",
     "shell.execute_reply": "2025-05-05T16:07:55.162845Z",
     "shell.execute_reply.started": "2025-05-05T16:07:52.232705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_Recommendations(df, model, tokenizer):\n",
    "    predicted = []\n",
    "    real = []\n",
    "    results = []\n",
    "    candidates = []\n",
    "    \n",
    "    for (i, row) in df.iterrows():\n",
    "            desc = row['Descriptions']\n",
    "            c = row['Candidates']\n",
    "            inputs = tokenizer([promptRec.format(desc, c, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=1000\n",
    "            )\n",
    "            response = tokenizer.batch_decode(outputs)\n",
    "            result = response[0].split(\"### Response:\")[1]\n",
    "            match = re.search(r'Ranked News Articles\\s*:\\s*(.*)', result)\n",
    "            if match:\n",
    "                after_phrase = match.group(1)\n",
    "                cs = re.findall(r'C\\d+', after_phrase)\n",
    "\n",
    "            cs = list(dict.fromkeys(re.findall(r'C\\d+', after_phrase) + re.findall(r'C\\d+', c)))\n",
    "\n",
    "            print(i)\n",
    "                                    \n",
    "            predicted.append(cs)\n",
    "            real.append(row['Labels'])\n",
    "            results.append(result)\n",
    "            candidates.append(c)\n",
    "\n",
    "            \n",
    "    return predicted, real, candidates, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a692859",
   "metadata": {},
   "source": [
    "### **A - Generate Descriptions :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,d,c,l = generate_Descriptions(test_df, model_d, tokenizer_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"history\":h,\n",
    "    \"Descriptions\":d,\n",
    "    \"Candidates\":c,\n",
    "    \"Labels\":l\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Descriptions'] = df['Descriptions'].str.replace('\\n\\n', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8994a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/MIND-Preprocessed/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5fe667",
   "metadata": {},
   "source": [
    "### **B - Generate Recommendations :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22425ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T16:07:55.164988Z",
     "iopub.status.busy": "2025-05-05T16:07:55.164598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predicted, real, candidates, results = generate_Recommendations(df, model_r, tokenizer_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90983a97",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recommendations = pd.DataFrame({\n",
    "    \"Predicted\": predicted,\n",
    "    \"Real\": real,\n",
    "    \"Candidates\" : candidates,\n",
    "    \"Results\" : results\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b2bcd-0cee-4d7d-8559-98c61555cfe7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "recommendations.to_csv(\"Outputs/Output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f13f3-21a6-4518-9b0e-bb0469695427",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mrr_score = mean_reciprocal_rank(recommendations[\"Predicted\"], recommendations[\"Real\"])\n",
    "ndcg_score_5 = ndcg_at_k(recommendations[\"Predicted\"], recommendations[\"Real\"], k=5)\n",
    "ndcg_score_10 = ndcg_at_k(recommendations[\"Predicted\"], recommendations[\"Real\"], k=10)\n",
    "_id = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1966ca-ee04-4332-a5f5-daf063144678",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    \"ID\": [_id],\n",
    "    \"MRR\": [round(mrr_score,2)],\n",
    "    \"NDCG@5\":[round(ndcg_score_5,2)],\n",
    "    \"NDCG@10\": [round(ndcg_score_10,2)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a2c3b-e304-4f32-b293-05fc295342ac",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "old_metrics = pd.read_csv('Results/metrics.csv', index_col = 0)\n",
    "metrics = pd.concat([old_metrics, metrics], axis=0, ignore_index=True)\n",
    "metrics.to_csv('Results/metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7252148,
     "sourceId": 11685247,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7334347,
     "sourceId": 11685610,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7334741,
     "sourceId": 11686211,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
