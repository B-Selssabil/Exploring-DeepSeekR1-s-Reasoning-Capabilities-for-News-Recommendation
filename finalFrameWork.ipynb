{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa98db77",
   "metadata": {},
   "source": [
    "### **1 - Installing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763763fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ad5a6",
   "metadata": {},
   "source": [
    "### **2 - Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bfa3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574891b",
   "metadata": {},
   "source": [
    "### **3 - Loading Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "# general configuration\n",
    "HGF = config['general']['HGF']\n",
    "\n",
    "# output models\n",
    "output_model_local_desc = config['outputs']['output_model_online_Desc']\n",
    "output_model_local_rec = config['outputs']['output_model_local_Rec']\n",
    "\n",
    "# model params\n",
    "max_seq_length = config['model']['max_seq_length']\n",
    "load_in_4bit = config['model']['load_in_4bit']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b3db0",
   "metadata": {},
   "source": [
    "### **4 - Loading Framework Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6474cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_desc, tokenizer_desc = FastLanguageModel.from_pretrained(\n",
    "    model_name = output_model_local_desc,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = HGF\n",
    ")\n",
    "\n",
    "model_rec, tokenizer_rec = FastLanguageModel.from_pretrained(\n",
    "    model_name = output_model_local_desc,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = HGF\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a799b2fb",
   "metadata": {},
   "source": [
    "### **5 - Defining Prompt Template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptDesc = \"\"\" Below is an instruction that describes a task, paired with an input that provied further context.\n",
    "Write a response that appropiately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are an interests analyzer. Based on the following user history, analyze their reading habits and generate a description of what kind of news articles they might be interested in reading next. \n",
    "\n",
    "### History:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "Description : \\n\n",
    "{}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b814a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptRec = \"\"\" Below is an instruction that describes a task, paired with an input that provied further context.\n",
    "Write a response that appropiately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You serve as a personalized news article recommendation system. Based on the user's preference descriptions below and the candidate articles, rank the candidates using their labels.\n",
    "Output Format:\n",
    "Ranked News Articles: <START> C#, C#, ..., C# <END>\n",
    "\n",
    "### Preferences Description:\n",
    "{}\n",
    "\n",
    "### Candidates:\n",
    "{}\n",
    "\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "Ranked News Articles : {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b53c1",
   "metadata": {},
   "source": [
    "### **6 - Assembling the Framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDescription(history, model, tokenizer):\n",
    "    inputs = tokenizer([promptDesc.format(history, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "    outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens= 900\n",
    "            )\n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    description = response[0].split(\"### Response:\")[1].split(\"\\nDescription : \\n\\n\\n\\n\")[1].replace(\"<｜end▁of▁sentence｜>\",\"\")\n",
    "    return description\n",
    "\n",
    "def recommendNews(description, candidates, model, tokenizer):\n",
    "\n",
    "    inputs = tokenizer([promptRec.format(description, candidates, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=1000\n",
    "            )\n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    result = response[0].split(\"### Response:\")[1]\n",
    "    cot = re.findall(r'<think>(.*?)</think>', result, re.DOTALL)\n",
    "    match = re.search(r'Ranked News Articles\\s*:\\s*(.*)', result)\n",
    "    if match:\n",
    "            after_phrase = match.group(1)\n",
    "            cs = re.findall(r'C\\d+', after_phrase)\n",
    "\n",
    "            rankedArticles  = list(dict.fromkeys(re.findall(r'C\\d+', after_phrase) + re.findall(r'C\\d+', candidates)))\n",
    "\n",
    "    return cot, rankedArticles\n",
    "\n",
    "def FrameWork(history, candidates):\n",
    "     \n",
    "    description = generateDescription(history, model_desc, tokenizer_desc)\n",
    "    cot, rankedArticles = recommendNews(description, candidates , model_rec ,tokenizer_rec)\n",
    "\n",
    "    return description, cot, rankedArticles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154691d5",
   "metadata": {},
   "source": [
    "### **7 - Using the Framewrok**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/MIND-Preprocessed/test.csv\")\n",
    "instance = df.iloc[3]\n",
    "history = instance['history']\n",
    "canidates = instance['canidates']\n",
    "label = instance['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "discription, cot, ranked_Articles = FrameWork(history, canidates)\n",
    "\n",
    "print(f\"History of user : \\n {history} \\n\")\n",
    "print(f\"Description generated : \\n{discription} \\n\")\n",
    "print(f\"Chain of thought generated : \\n{cot} \\n\")\n",
    "print(f\"The correct next article : \\n{label} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
